{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a746716-6811-46c3-ae40-a946ce5470a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing test_aug_select.csv: 100%|██████████████████████████████████████████████| 2210/2210 [04:33<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TLTTA_enhanced_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "mlm_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "mlm_bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "argostranslate.package.update_package_index()\n",
    "\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "\n",
    "en_to_zh = next(filter(lambda x: x.from_code == \"en\" and x.to_code == \"zh\", available_packages))\n",
    "argostranslate.package.install_from_path(en_to_zh.download())\n",
    "\n",
    "zh_to_en = next(filter(lambda x: x.from_code == \"zh\" and x.to_code == \"en\", available_packages))\n",
    "argostranslate.package.install_from_path(zh_to_en.download())\n",
    "\n",
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if get_wordnet_pos(pos) == syn.pos():\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return synonyms\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    replacement_types = ['N', 'J'] \n",
    "\n",
    "    for random_word, pos in pos_tags:\n",
    "        if pos.startswith(tuple(replacement_types)):\n",
    "            synonyms = get_synonyms(random_word, pos)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= n:\n",
    "                break\n",
    "\n",
    "   \n",
    "    if num_replaced < n:\n",
    "        for random_word, pos in pos_tags:\n",
    "            if not pos.startswith(tuple(replacement_types)):  \n",
    "                synonyms = get_synonyms(random_word, pos)\n",
    "                if len(synonyms) >= 1:\n",
    "                    synonym = random.choice(list(synonyms))\n",
    "                    new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                    num_replaced += 1\n",
    "                if num_replaced >= n:\n",
    "                    break\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_insertion(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "\n",
    "    max_inserts = max(1, int(len(words) * 0.1))  \n",
    "    n = min(n, max_inserts)\n",
    "    \n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def add_word(new_words):\n",
    "    random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "    synonyms = get_synonyms(random_word, 'n')\n",
    "    if len(synonyms) < 1:\n",
    "        synonyms = [random_word]\n",
    "    random_synonym = random.choice(list(synonyms))\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def bert_embedding_replacement(sentence, top_k=5, replace_rate=0.15):\n",
    "    words = sentence.split()\n",
    "    num_words_to_replace = max(1, int(len(words) * replace_rate))\n",
    "    words_to_replace = random.sample(range(len(words)), num_words_to_replace)\n",
    "\n",
    "    new_words = words.copy()\n",
    "\n",
    "    for idx in words_to_replace:\n",
    "        masked_sentence = words.copy()\n",
    "        masked_sentence[idx] = '[MASK]'\n",
    "        masked_sentence = ' '.join(masked_sentence)\n",
    "\n",
    "        inputs = mlm_tokenizer(masked_sentence, return_tensors='pt')\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == mlm_tokenizer.mask_token_id)[1]\n",
    "        token_logits = mlm_bert_model(**inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "        new_word = random.choice([mlm_tokenizer.decode([token]).strip() for token in top_k_tokens])\n",
    "        new_words[idx] = new_word\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def tfidf_augmentation(sentences, augmentation_rate=0.05):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "\n",
    "    def augment_sentence(sentence):\n",
    "        try:\n",
    "            words = sentence.split()\n",
    "            num_words_to_augment = max(1, int(len(words) * augmentation_rate))\n",
    "            tfidf_scores = vectorizer.transform([sentence]).toarray().flatten()\n",
    "            word_scores = [(word, tfidf_scores[features.tolist().index(word)]) \n",
    "                           for word in words if word in features]\n",
    "            word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            words_to_augment = [word for word, score in word_scores[:num_words_to_augment]]\n",
    "\n",
    "            new_sentence = []\n",
    "            for word in words:\n",
    "                if word in words_to_augment:\n",
    "                    synonyms = wordnet.synsets(word)\n",
    "                    if synonyms:\n",
    "                        synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "                        new_sentence.append(synonym.replace('_', ' '))\n",
    "                    else:\n",
    "                        new_sentence.append(word)\n",
    "                else:\n",
    "                    new_sentence.append(word)\n",
    "\n",
    "            return ' '.join(new_sentence)\n",
    "        except Exception as e:\n",
    "            return sentence  \n",
    "\n",
    "    return [augment_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def argos_back_translation(sentence):\n",
    "   \n",
    "    chinese_text = argostranslate.translate.translate(sentence, 'en', 'zh')\n",
    "    back_to_english = argostranslate.translate.translate(chinese_text, 'zh', 'en')\n",
    "    return back_to_english\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    pred_df = pd.read_csv(input_file)\n",
    "\n",
    "    enhanced_data = []\n",
    "    sentences = pred_df['sentence'].tolist()\n",
    "    tfidf_augmented_sentences = tfidf_augmentation(sentences)\n",
    "\n",
    "    augmentations = [\n",
    "        synonym_replacement,\n",
    "        argos_back_translation,\n",
    "        random_insertion,\n",
    "        bert_embedding_replacement,\n",
    "        lambda x: tfidf_augmented_sentences[sentences.index(x)]\n",
    "    ]\n",
    "\n",
    "    for index, row in tqdm(pred_df.iterrows(), total=pred_df.shape[0], desc=f\"Processing {input_file}\"):\n",
    "        original_sentence = row['sentence']\n",
    "        label = row['label']\n",
    "\n",
    "        aug_01 = augmentations[row['aug_select_01']](original_sentence)\n",
    "        aug_02 = augmentations[row['aug_select_02']](original_sentence)\n",
    "\n",
    "        enhanced_data.append([original_sentence, label])\n",
    "        enhanced_data.append([aug_01, label])\n",
    "        enhanced_data.append([aug_02, label])\n",
    "\n",
    "    enhanced_df = pd.DataFrame(enhanced_data, columns=['sentence', 'label'])\n",
    "    enhanced_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {output_file}\")\n",
    "\n",
    "#process_file('test_aug_pre_T.csv', 'T_TTA_enhanced_test.csv')\n",
    "#process_file('test_aug_pre_R.csv', 'R_TTA_enhanced_test.csv')\n",
    "process_file('test_aug_select.csv', 'TLTTA_enhanced_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d784a-f63e-433f-bed2-a2c78d2a9fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab48c8-9402-42b8-833e-60eef1a97422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
