{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25928f0-bf64-4a18-b14c-9becdcacac74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process train.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing sentences for train.csv: 100%|█████████████████████████████████████████| 8544/8544 [1:03:56<00:00,  2.23it/s]\n",
      "Selecting best augmentations for train.csv: 100%|████████████████████████████████| 8544/8544 [00:01<00:00, 4590.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process validation.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing sentences for validation.csv: 100%|██████████████████████████████████████| 1101/1101 [08:14<00:00,  2.23it/s]\n",
      "Selecting best augmentations for validation.csv: 100%|███████████████████████████| 1101/1101 [00:00<00:00, 4528.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process test.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing sentences for test.csv: 100%|████████████████████████████████████████████| 2210/2210 [16:24<00:00,  2.24it/s]\n",
      "Selecting best augmentations for test.csv: 100%|█████████████████████████████████| 2210/2210 [00:00<00:00, 4565.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer,RobertaForMaskedLM,RobertaForSequenceClassification\n",
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "mlm_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "mlm_bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "argostranslate.package.update_package_index()\n",
    "\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "\n",
    "en_to_zh = next(filter(lambda x: x.from_code == \"en\" and x.to_code == \"zh\", available_packages))\n",
    "argostranslate.package.install_from_path(en_to_zh.download())\n",
    "\n",
    "zh_to_en = next(filter(lambda x: x.from_code == \"zh\" and x.to_code == \"en\", available_packages))\n",
    "argostranslate.package.install_from_path(zh_to_en.download())\n",
    "\n",
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        if get_wordnet_pos(pos) == syn.pos():\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return synonyms\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    replacement_types = ['N', 'J']  \n",
    "\n",
    "    for random_word, pos in pos_tags:\n",
    "        if pos.startswith(tuple(replacement_types)):\n",
    "            synonyms = get_synonyms(random_word, pos)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= n:\n",
    "                break\n",
    "\n",
    "    if num_replaced < n:\n",
    "        for random_word, pos in pos_tags:\n",
    "            if not pos.startswith(tuple(replacement_types)):  \n",
    "                synonyms = get_synonyms(random_word, pos)\n",
    "                if len(synonyms) >= 1:\n",
    "                    synonym = random.choice(list(synonyms))\n",
    "                    new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                    num_replaced += 1\n",
    "                if num_replaced >= n:\n",
    "                    break\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_insertion(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "\n",
    "    max_inserts = max(1, int(len(words) * 0.1))  \n",
    "    n = min(n, max_inserts)\n",
    "    \n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def add_word(new_words):\n",
    "    random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "    synonyms = get_synonyms(random_word, 'n')\n",
    "    if len(synonyms) < 1:\n",
    "        synonyms = [random_word]\n",
    "    random_synonym = random.choice(list(synonyms))\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def bert_embedding_replacement(sentence, top_k=5, replace_rate=0.15):\n",
    "    words = sentence.split()\n",
    "    num_words_to_replace = max(1, int(len(words) * replace_rate))\n",
    "    words_to_replace = random.sample(range(len(words)), num_words_to_replace)\n",
    "\n",
    "    new_words = words.copy()\n",
    "\n",
    "    for idx in words_to_replace:\n",
    "        masked_sentence = words.copy()\n",
    "        masked_sentence[idx] = '[MASK]'\n",
    "        masked_sentence = ' '.join(masked_sentence)\n",
    "\n",
    "        inputs = mlm_tokenizer(masked_sentence, return_tensors='pt')\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == mlm_tokenizer.mask_token_id)[1]\n",
    "        token_logits = mlm_bert_model(**inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "        new_word = random.choice([mlm_tokenizer.decode([token]).strip() for token in top_k_tokens])\n",
    "        new_words[idx] = new_word\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def tfidf_augmentation(sentences, augmentation_rate=0.05):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "\n",
    "    def augment_sentence(sentence):\n",
    "        try:\n",
    "            words = sentence.split()\n",
    "            num_words_to_augment = max(1, int(len(words) * augmentation_rate))\n",
    "            tfidf_scores = vectorizer.transform([sentence]).toarray().flatten()\n",
    "            word_scores = [(word, tfidf_scores[features.tolist().index(word)]) \n",
    "                           for word in words if word in features]\n",
    "            word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            words_to_augment = [word for word, score in word_scores[:num_words_to_augment]]\n",
    "\n",
    "            new_sentence = []\n",
    "            for word in words:\n",
    "                if word in words_to_augment:\n",
    "                    synonyms = wordnet.synsets(word)\n",
    "                    if synonyms:\n",
    "                        synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "                        new_sentence.append(synonym.replace('_', ' '))\n",
    "                    else:\n",
    "                        new_sentence.append(word)\n",
    "                else:\n",
    "                    new_sentence.append(word)\n",
    "\n",
    "            return ' '.join(new_sentence)\n",
    "        except Exception as e:\n",
    "            return sentence \n",
    "    return [augment_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "def argos_back_translation(sentence):\n",
    "   \n",
    "    chinese_text = argostranslate.translate.translate(sentence, 'en', 'zh')\n",
    "    back_to_english = argostranslate.translate.translate(chinese_text, 'zh', 'en')\n",
    "    return back_to_english\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)\n",
    "model.load_state_dict(torch.load('Labeling_Roberta_model.bin'))\n",
    "model.to('cuda')\n",
    "\n",
    "def get_predictions(sentence, label):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probs[0][label].item()\n",
    "    \n",
    "def process_data(file_name, aug_select_file_name):\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    sentences = df['sentence'].tolist()\n",
    "    tfidf_augmented_sentences = tfidf_augmentation(sentences)\n",
    "\n",
    "    augmented_data = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Enhancing sentences for {file_name}\"):\n",
    "        sentence = row['sentence']\n",
    "        label = row['label']\n",
    "        \n",
    "        augmentations = [\n",
    "            synonym_replacement,\n",
    "            argos_back_translation,\n",
    "            random_insertion,\n",
    "            bert_embedding_replacement,\n",
    "            lambda x: tfidf_augmented_sentences[index]  \n",
    "        ]\n",
    "\n",
    "        for aug_id, augment in enumerate(augmentations):\n",
    "            augmented_sentence = augment(sentence)\n",
    "            probability = get_predictions(augmented_sentence, label)\n",
    "            augmented_data.append([sentence, augmented_sentence, label, aug_id, probability])\n",
    "\n",
    "    aug_df = pd.DataFrame(augmented_data, columns=['original_sentence','augmented_sentence', 'label', 'aug_select', 'probability'])\n",
    " \n",
    "    best_augmentations = []\n",
    "    aug_df['group_id'] = aug_df.index // 5\n",
    "    \n",
    "    for group_id, group in tqdm(aug_df.groupby('group_id'), total=aug_df['group_id'].nunique(), desc=f\"Selecting best augmentations for {file_name}\"):\n",
    "        sorted_group = group.sort_values(by='probability', ascending=False)\n",
    "        best_augmentations.append([\n",
    "            sorted_group.iloc[0]['original_sentence'],  \n",
    "            sorted_group.iloc[0]['label'],\n",
    "            sorted_group.iloc[0]['aug_select'],\n",
    "            sorted_group.iloc[1]['aug_select']\n",
    "        ])\n",
    "\n",
    "    final_df = pd.DataFrame(best_augmentations, columns=['sentence', 'label', 'aug_select_01', 'aug_select_02'])\n",
    "    final_df.to_csv(aug_select_file_name, index=False, encoding='utf-8')\n",
    "\n",
    "print('process test.csv...')\n",
    "process_data('test.csv', 'test_aug_select.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a90f1-4a32-48da-9b4a-65ef40815930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
