{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb50a80-58a8-476f-8cbb-6115c9da9f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.roberta.embeddings.word_embeddings.weight: False\n",
      "roberta.roberta.embeddings.position_embeddings.weight: False\n",
      "roberta.roberta.embeddings.token_type_embeddings.weight: False\n",
      "roberta.roberta.embeddings.LayerNorm.weight: False\n",
      "roberta.roberta.embeddings.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.0.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.0.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.0.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.0.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.0.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.0.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.0.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.0.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.0.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.1.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.1.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.1.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.1.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.1.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.1.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.1.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.1.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.1.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.2.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.2.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.2.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.2.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.2.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.2.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.2.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.2.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.2.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.3.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.3.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.3.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.3.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.3.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.3.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.3.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.3.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.3.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.4.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.4.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.4.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.4.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.4.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.4.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.4.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.4.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.4.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.5.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.5.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.5.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.5.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.5.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.5.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.5.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.5.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.5.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.6.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.6.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.6.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.6.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.6.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.6.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.6.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.6.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.6.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.7.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.7.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.7.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.7.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.7.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.7.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.7.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.7.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.7.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.8.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.8.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.8.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.8.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.8.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.8.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.8.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.8.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.8.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.9.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.9.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.9.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.9.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.9.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.9.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.9.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.9.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.9.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.9.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.9.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.10.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.10.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.10.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.10.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.10.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.10.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.10.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.10.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.10.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.10.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.10.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.11.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.11.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.11.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.11.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.11.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.11.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.11.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.11.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.11.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.11.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.11.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.12.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.12.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.12.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.12.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.12.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.12.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.12.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.12.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.12.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.12.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.12.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.13.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.13.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.13.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.13.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.13.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.13.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.13.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.13.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.13.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.13.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.13.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.14.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.14.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.14.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.14.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.14.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.14.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.14.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.14.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.14.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.14.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.14.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.15.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.15.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.15.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.15.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.15.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.15.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.15.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.15.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.15.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.15.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.15.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.16.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.16.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.16.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.16.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.16.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.16.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.16.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.16.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.16.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.16.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.16.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.17.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.17.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.17.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.17.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.17.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.17.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.17.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.17.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.17.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.17.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.17.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.18.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.18.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.18.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.18.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.18.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.18.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.18.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.18.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.18.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.18.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.18.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.query.weight: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.query.bias: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.key.weight: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.key.bias: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.value.weight: False\n",
      "roberta.roberta.encoder.layer.19.attention.self.value.bias: False\n",
      "roberta.roberta.encoder.layer.19.attention.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.19.attention.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.19.attention.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.19.attention.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.19.intermediate.dense.weight: False\n",
      "roberta.roberta.encoder.layer.19.intermediate.dense.bias: False\n",
      "roberta.roberta.encoder.layer.19.output.dense.weight: False\n",
      "roberta.roberta.encoder.layer.19.output.dense.bias: False\n",
      "roberta.roberta.encoder.layer.19.output.LayerNorm.weight: False\n",
      "roberta.roberta.encoder.layer.19.output.LayerNorm.bias: False\n",
      "roberta.roberta.encoder.layer.20.attention.self.query.weight: True\n",
      "roberta.roberta.encoder.layer.20.attention.self.query.bias: True\n",
      "roberta.roberta.encoder.layer.20.attention.self.key.weight: True\n",
      "roberta.roberta.encoder.layer.20.attention.self.key.bias: True\n",
      "roberta.roberta.encoder.layer.20.attention.self.value.weight: True\n",
      "roberta.roberta.encoder.layer.20.attention.self.value.bias: True\n",
      "roberta.roberta.encoder.layer.20.attention.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.20.attention.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.20.attention.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.20.attention.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.20.intermediate.dense.weight: True\n",
      "roberta.roberta.encoder.layer.20.intermediate.dense.bias: True\n",
      "roberta.roberta.encoder.layer.20.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.20.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.20.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.20.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.query.weight: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.query.bias: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.key.weight: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.key.bias: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.value.weight: True\n",
      "roberta.roberta.encoder.layer.21.attention.self.value.bias: True\n",
      "roberta.roberta.encoder.layer.21.attention.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.21.attention.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.21.attention.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.21.attention.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.21.intermediate.dense.weight: True\n",
      "roberta.roberta.encoder.layer.21.intermediate.dense.bias: True\n",
      "roberta.roberta.encoder.layer.21.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.21.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.21.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.21.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.query.weight: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.query.bias: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.key.weight: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.key.bias: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.value.weight: True\n",
      "roberta.roberta.encoder.layer.22.attention.self.value.bias: True\n",
      "roberta.roberta.encoder.layer.22.attention.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.22.attention.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.22.attention.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.22.attention.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.22.intermediate.dense.weight: True\n",
      "roberta.roberta.encoder.layer.22.intermediate.dense.bias: True\n",
      "roberta.roberta.encoder.layer.22.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.22.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.22.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.22.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.query.weight: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.query.bias: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.key.weight: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.key.bias: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.value.weight: True\n",
      "roberta.roberta.encoder.layer.23.attention.self.value.bias: True\n",
      "roberta.roberta.encoder.layer.23.attention.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.23.attention.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.23.attention.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.23.attention.output.LayerNorm.bias: True\n",
      "roberta.roberta.encoder.layer.23.intermediate.dense.weight: True\n",
      "roberta.roberta.encoder.layer.23.intermediate.dense.bias: True\n",
      "roberta.roberta.encoder.layer.23.output.dense.weight: True\n",
      "roberta.roberta.encoder.layer.23.output.dense.bias: True\n",
      "roberta.roberta.encoder.layer.23.output.LayerNorm.weight: True\n",
      "roberta.roberta.encoder.layer.23.output.LayerNorm.bias: True\n",
      "roberta.classifier.dense.weight: True\n",
      "roberta.classifier.dense.bias: True\n",
      "roberta.classifier.out_proj.weight: True\n",
      "roberta.classifier.out_proj.bias: True\n",
      "classifier_01.0.weight: True\n",
      "classifier_01.0.bias: True\n",
      "classifier_01.3.weight: True\n",
      "classifier_01.3.bias: True\n",
      "classifier_02.0.weight: True\n",
      "classifier_02.0.bias: True\n",
      "classifier_02.3.weight: True\n",
      "classifier_02.3.bias: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 94.1971, Train Accuracy: 0.2316, Validation Loss: 88.7898, Validation Accuracy: 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:52<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 87.4786, Train Accuracy: 0.2310, Validation Loss: 73.9821, Validation Accuracy: 0.2071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:52<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 67.9198, Train Accuracy: 0.2313, Validation Loss: 39.5352, Validation Accuracy: 0.2157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:52<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 44.6766, Train Accuracy: 0.2124, Validation Loss: 28.1097, Validation Accuracy: 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 25.7151, Train Accuracy: 0.2134, Validation Loss: 30.4442, Validation Accuracy: 0.2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 19.9565, Train Accuracy: 0.2151, Validation Loss: 26.0718, Validation Accuracy: 0.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 18.0775, Train Accuracy: 0.2181, Validation Loss: 24.5068, Validation Accuracy: 0.2348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 16.2598, Train Accuracy: 0.2223, Validation Loss: 21.4523, Validation Accuracy: 0.2389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 14.4306, Train Accuracy: 0.2206, Validation Loss: 19.4116, Validation Accuracy: 0.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 13.0039, Train Accuracy: 0.2169, Validation Loss: 18.0945, Validation Accuracy: 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 11.4000, Train Accuracy: 0.2145, Validation Loss: 14.9037, Validation Accuracy: 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 10.2854, Train Accuracy: 0.2157, Validation Loss: 12.3314, Validation Accuracy: 0.2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 9.1709, Train Accuracy: 0.2173, Validation Loss: 9.9823, Validation Accuracy: 0.2243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 7.9129, Train Accuracy: 0.2185, Validation Loss: 8.8484, Validation Accuracy: 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 6.8746, Train Accuracy: 0.2197, Validation Loss: 7.3304, Validation Accuracy: 0.2171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 6.0404, Train Accuracy: 0.2238, Validation Loss: 6.0026, Validation Accuracy: 0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 5.2054, Train Accuracy: 0.2166, Validation Loss: 4.9086, Validation Accuracy: 0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 4.5366, Train Accuracy: 0.2257, Validation Loss: 4.3788, Validation Accuracy: 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:53<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 4.0635, Train Accuracy: 0.2235, Validation Loss: 3.8139, Validation Accuracy: 0.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 3.7357, Train Accuracy: 0.2251, Validation Loss: 3.5735, Validation Accuracy: 0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 3.5081, Train Accuracy: 0.2330, Validation Loss: 3.3694, Validation Accuracy: 0.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 3.3983, Train Accuracy: 0.2302, Validation Loss: 3.3351, Validation Accuracy: 0.2284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train Loss: 3.3211, Train Accuracy: 0.2353, Validation Loss: 3.2702, Validation Accuracy: 0.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train Loss: 3.2522, Train Accuracy: 0.2405, Validation Loss: 3.2450, Validation Accuracy: 0.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 3.2302, Train Accuracy: 0.2383, Validation Loss: 3.2196, Validation Accuracy: 0.2275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train Loss: 3.2182, Train Accuracy: 0.2412, Validation Loss: 3.2219, Validation Accuracy: 0.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train Loss: 3.2121, Train Accuracy: 0.2377, Validation Loss: 3.1941, Validation Accuracy: 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Train Loss: 3.2134, Train Accuracy: 0.2445, Validation Loss: 3.1984, Validation Accuracy: 0.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 3.1940, Train Accuracy: 0.2408, Validation Loss: 3.2171, Validation Accuracy: 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 267/267 [01:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Train Loss: 3.1870, Train Accuracy: 0.2446, Validation Loss: 3.1744, Validation Accuracy: 0.2280\n",
      "Predictions saved to 'test_aug_pre_M.csv'\n",
      "Classifier 01 Accuracy: 0.2158\n",
      "Classifier 02 Accuracy: 0.2502\n",
      "Average Accuracy: 0.2330\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, AdamW,get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_data = pd.read_csv('train_aug_select.csv')\n",
    "validation_data = pd.read_csv('validation_aug_select.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.sentences = data['sentence'].values\n",
    "        self.labels_01 = data['aug_select_01'].values\n",
    "        self.labels_02 = data['aug_select_02'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences[idx])\n",
    "        inputs = self.tokenizer(sentence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_01 = torch.tensor(self.labels_01[idx], dtype=torch.long)\n",
    "        label_02 = torch.tensor(self.labels_02[idx], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label_01, label_02\n",
    "\n",
    "class AugmentationSelectorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AugmentationSelectorModel, self).__init__()\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=5)\n",
    "        for name, param in self.roberta.named_parameters():\n",
    "            if \"encoder.layer.\" in name and int(name.split(\".\")[3]) < 20:\n",
    "                param.requires_grad = False  \n",
    "\n",
    "        for param in self.roberta.roberta.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.classifier_01 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "\n",
    "        self.classifier_02 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = torch.sum(sequence_output,dim=1)\n",
    "        out_01 = self.classifier_01(pooled_output)\n",
    "        out_02 = self.classifier_02(pooled_output)\n",
    "        return out_01, out_02\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "train_dataset = AugmentationDataset(train_data, tokenizer)\n",
    "validation_dataset = AugmentationDataset(validation_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AugmentationSelectorModel().to(device)  \n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=4e-5, correct_bias=False, weight_decay=1e-2)\n",
    "\n",
    "total_steps = len(train_loader) * 30  \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def train_model(model, train_loader, validation_loader, epochs=30):\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_01 = 0\n",
    "        correct_02 = 0\n",
    "        total = 0\n",
    "\n",
    "        for input_ids, attention_mask, label_01, label_02 in tqdm(train_loader):\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            label_01, label_02 = label_01.to(device), label_02.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out_01, out_02 = model(input_ids, attention_mask)\n",
    "            loss_01 = criterion(out_01, label_01)\n",
    "            loss_02 = criterion(out_02, label_02)\n",
    "\n",
    "            loss = loss_01 + loss_02  \n",
    "            loss.backward()  \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n",
    "            optimizer.step()  \n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds_01 = torch.argmax(out_01, dim=1)\n",
    "            preds_02 = torch.argmax(out_02, dim=1)\n",
    "            correct_01 += (preds_01 == label_01).sum().item()\n",
    "            correct_02 += (preds_02 == label_02).sum().item()\n",
    "            total += label_01.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy_01 = correct_01 / total\n",
    "        train_accuracy_02 = correct_02 / total\n",
    "        train_accuracy = (train_accuracy_01 + train_accuracy_02) / 2\n",
    "       \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct_01 = 0\n",
    "        val_correct_02 = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, label_01, label_02 in validation_loader:\n",
    "                input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "                label_01, label_02 = label_01.to(device), label_02.to(device)\n",
    "\n",
    "                out_01, out_02 = model(input_ids, attention_mask)\n",
    "                loss_01 = criterion(out_01, label_01)\n",
    "                loss_02 = criterion(out_02, label_02)\n",
    "                val_loss += (loss_01 + loss_02).item()\n",
    "\n",
    "                val_correct_01 += (torch.argmax(out_01, dim=1) == label_01).sum().item()\n",
    "                val_correct_02 += (torch.argmax(out_02, dim=1) == label_02).sum().item()\n",
    "                val_total += label_01.size(0)\n",
    "\n",
    "        val_loss /= len(validation_loader)\n",
    "        val_accuracy_01 = val_correct_01 / val_total\n",
    "        val_accuracy_02 = val_correct_02 / val_total\n",
    "        val_accuracy = (val_accuracy_01 + val_accuracy_02) / 2\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'Test_aug_select_model.pth')\n",
    "           \n",
    "train_model(model, train_loader, validation_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('Test_aug_select_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_sentences = test_data['sentence'].values\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        out_01, out_02 = model(input_ids, attention_mask)\n",
    "        pred_01 = torch.argmax(out_01, dim=1).item()\n",
    "        pred_02 = torch.argmax(out_02, dim=1).item()\n",
    "\n",
    "        predictions.append([sentence, test_labels[i], pred_01, pred_02])\n",
    "\n",
    "output_df = pd.DataFrame(predictions, columns=['sentence', 'label', 'aug_select_01', 'aug_select_02'])\n",
    "output_df.to_csv('test_aug_pre_M.csv', index=False)\n",
    "print(\"Predictions saved to 'test_aug_pre_M.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c44c3-11ad-4a50-9f6e-ba279f7232a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e19aa-ab1c-40a2-bfdf-9939f73a1a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
